/**
 * PTO Runtime2 - Scheduler Implementation
 *
 * Implements scheduler state management, ready queues, and task lifecycle.
 *
 * Based on: docs/runtime_buffer_manager_methods.md
 */

#include "pto_scheduler.h"
#include <inttypes.h>
#include <stdlib.h>
#include <string.h>
#include "aicpu/device_time.h"
#include "common/unified_log.h"

// Weak fallback for non-AICPU builds.
__attribute__((weak)) uint64_t get_sys_cnt_aicpu() { return 0; }

// =============================================================================
// Task State Names
// =============================================================================

const char* pto2_task_state_name(PTO2TaskState state) {
    switch (state) {
        case PTO2_TASK_PENDING:   return "PENDING";
        case PTO2_TASK_READY:     return "READY";
        case PTO2_TASK_RUNNING:   return "RUNNING";
        case PTO2_TASK_COMPLETED: return "COMPLETED";
        case PTO2_TASK_CONSUMED:  return "CONSUMED";
        default:                  return "UNKNOWN";
    }
}

// =============================================================================
// Ready Queue Implementation
// =============================================================================

bool pto2_ready_queue_init(PTO2ReadyQueue* queue, uint64_t capacity) {
    queue->task_ids = (int32_t*)malloc(capacity * sizeof(int32_t));
    if (!queue->task_ids) {
        return false;
    }

    queue->head = 0;
    queue->tail = 0;
    queue->capacity = capacity;
    queue->count = 0;
    queue->spinlock = 0;

    return true;
}

void pto2_ready_queue_destroy(PTO2ReadyQueue* queue) {
    if (queue->task_ids) {
        free(queue->task_ids);
        queue->task_ids = NULL;
    }
}

void pto2_ready_queue_reset(PTO2ReadyQueue* queue) {
    queue->head = 0;
    queue->tail = 0;
    queue->count = 0;
}

bool pto2_ready_queue_push(PTO2ReadyQueue* queue, int32_t task_id,
                            uint64_t* wait_cycles,
                            uint64_t* hold_cycles) {
    bool profile = (wait_cycles != nullptr || hold_cycles != nullptr);
    uint64_t t0 = 0, t1 = 0, t2 = 0;
    if (profile) t0 = get_sys_cnt_aicpu();

    while (__atomic_exchange_n(&queue->spinlock, 1, __ATOMIC_ACQUIRE)) {
        PTO2_SPIN_PAUSE_LIGHT();
    }
    if (profile) t1 = get_sys_cnt_aicpu();

    bool result = false;
    if (!pto2_ready_queue_full(queue)) {
        queue->task_ids[queue->tail] = task_id;
        queue->tail = (queue->tail + 1) % queue->capacity;
        queue->count++;
        result = true;
    }

    __atomic_store_n(&queue->spinlock, 0, __ATOMIC_RELEASE);
    if (profile) {
        t2 = get_sys_cnt_aicpu();
        if (wait_cycles) *wait_cycles += (t1 - t0);
        if (hold_cycles) *hold_cycles += (t2 - t1);
    }
    return result;
}

int32_t pto2_ready_queue_pop(PTO2ReadyQueue* queue,
                              uint64_t* wait_cycles,
                              uint64_t* hold_cycles,
                              bool* hit) {
    bool profile = (wait_cycles != nullptr || hold_cycles != nullptr);
    uint64_t t0 = 0, t1 = 0, t2 = 0;
    if (profile) t0 = get_sys_cnt_aicpu();

    while (__atomic_exchange_n(&queue->spinlock, 1, __ATOMIC_ACQUIRE)) {
        PTO2_SPIN_PAUSE_LIGHT();
    }
    if (profile) t1 = get_sys_cnt_aicpu();

    int32_t task_id = -1;
    if (!pto2_ready_queue_empty(queue)) {
        task_id = queue->task_ids[queue->head];
        queue->head = (queue->head + 1) % queue->capacity;
        queue->count--;
    }

    __atomic_store_n(&queue->spinlock, 0, __ATOMIC_RELEASE);
    if (hit) *hit = (task_id >= 0);
    if (profile) {
        t2 = get_sys_cnt_aicpu();
        if (wait_cycles) *wait_cycles += (t1 - t0);
        if (hold_cycles) *hold_cycles += (t2 - t1);
    }
    return task_id;
}

// =============================================================================
// Scheduler Initialization
// =============================================================================

bool pto2_scheduler_init(PTO2SchedulerState* sched,
                          PTO2SharedMemoryHandle* sm_handle,
                          PTO2DepListPool* dep_pool,
                          void* heap_base) {
    memset(sched, 0, sizeof(PTO2SchedulerState));

    sched->sm_handle = sm_handle;
    sched->dep_pool = dep_pool;
    sched->heap_base = heap_base;

    // Get runtime task_window_size from shared memory header
    uint64_t window_size = sm_handle->header->task_window_size;
    sched->task_window_size = window_size;
    sched->task_window_mask = window_size - 1;  // For fast modulo (window_size must be power of 2)

    // Initialize local copies of ring pointers
    sched->last_task_alive = 0;
    sched->heap_tail = 0;

    // Allocate per-task state arrays (dynamically sized based on runtime window_size)
    sched->task_state = (PTO2TaskState*)calloc(window_size, sizeof(PTO2TaskState));
    if (!sched->task_state) {
        return false;
    }

    sched->fanin_refcount = (int32_t*)calloc(window_size, sizeof(int32_t));
    if (!sched->fanin_refcount) {
        free(sched->task_state);
        return false;
    }

    sched->fanout_refcount = (int32_t*)calloc(window_size, sizeof(int32_t));
    if (!sched->fanout_refcount) {
        free(sched->fanin_refcount);
        free(sched->task_state);
        return false;
    }

    // Initialize ready queues
    for (int i = 0; i < PTO2_NUM_WORKER_TYPES; i++) {
        if (!pto2_ready_queue_init(&sched->ready_queues[i], PTO2_READY_QUEUE_SIZE)) {
            // Cleanup on failure
            for (int j = 0; j < i; j++) {
                pto2_ready_queue_destroy(&sched->ready_queues[j]);
            }
            free(sched->fanout_refcount);
            free(sched->fanin_refcount);
            free(sched->task_state);
            return false;
        }
    }

    return true;
}

void pto2_scheduler_destroy(PTO2SchedulerState* sched) {
    if (sched->task_state) {
        free(sched->task_state);
        sched->task_state = NULL;
    }

    if (sched->fanin_refcount) {
        free(sched->fanin_refcount);
        sched->fanin_refcount = NULL;
    }

    if (sched->fanout_refcount) {
        free(sched->fanout_refcount);
        sched->fanout_refcount = NULL;
    }

    for (int i = 0; i < PTO2_NUM_WORKER_TYPES; i++) {
        pto2_ready_queue_destroy(&sched->ready_queues[i]);
    }
}

void pto2_scheduler_reset(PTO2SchedulerState* sched) {
    sched->last_task_alive = 0;
    sched->heap_tail = 0;

    memset(sched->task_state, 0, sched->task_window_size * sizeof(PTO2TaskState));
    memset(sched->fanin_refcount, 0, sched->task_window_size * sizeof(int32_t));
    memset(sched->fanout_refcount, 0, sched->task_window_size * sizeof(int32_t));

    for (int i = 0; i < PTO2_NUM_WORKER_TYPES; i++) {
        pto2_ready_queue_reset(&sched->ready_queues[i]);
    }

    sched->tasks_completed = 0;
    sched->tasks_consumed = 0;
}

void pto2_scheduler_mark_running(PTO2SchedulerState* sched, int32_t task_id) {
    int32_t slot = sched->pto2_task_slot(task_id);
    sched->task_state[slot] = PTO2_TASK_RUNNING;
}

int32_t pto2_scheduler_get_ready_task(PTO2SchedulerState* sched,
                                       PTO2WorkerType worker_type,
                                       uint64_t* wait_cycles,
                                       uint64_t* hold_cycles,
                                       bool* hit) {
    return pto2_ready_queue_pop(&sched->ready_queues[worker_type], wait_cycles, hold_cycles, hit);
}

// =============================================================================
// Task Completion Handling
// =============================================================================

/**
 * Check if task can transition to CONSUMED and handle if so
 *
 * NOTE: fanout_refcount is accessed atomically because it can be modified
 * by both orchestrator thread (via scope_end) and scheduler thread (via task_complete).
 */
static void check_and_handle_consumed(PTO2SchedulerState* sched,
                                       int32_t task_id,
                                       PTO2TaskDescriptor* task) {
    int32_t slot = sched->pto2_task_slot(task_id);

    // Read fanout_count (set by orchestrator, only grows)
    int32_t fanout_count = __atomic_load_n(&task->fanout_count, __ATOMIC_ACQUIRE);

    // Read fanout_refcount atomically (modified by both orchestrator and scheduler threads)
    int32_t refcount = __atomic_load_n(&sched->fanout_refcount[slot], __ATOMIC_ACQUIRE);

    if (refcount != fanout_count) {
        return;  // Not all references released yet
    }

    // Use CAS to atomically transition COMPLETED -> CONSUMED
    // This prevents multiple threads from transitioning the same task
    PTO2TaskState expected = PTO2_TASK_COMPLETED;
    if (!__atomic_compare_exchange_n(&sched->task_state[slot], &expected, PTO2_TASK_CONSUMED,
                                      false, __ATOMIC_ACQ_REL, __ATOMIC_ACQUIRE)) {
        // CAS failed - either not COMPLETED or another thread already transitioned
        return;
    }

    // Successfully transitioned to CONSUMED
    __atomic_fetch_add(&sched->tasks_consumed, 1, __ATOMIC_RELAXED);

    // Reset refcounts for slot reuse (ring buffer will reuse this slot)
    // Use atomic store for fanout_refcount
    __atomic_store_n(&sched->fanout_refcount[slot], 0, __ATOMIC_RELEASE);
    __atomic_store_n(&sched->fanin_refcount[slot], 0, __ATOMIC_RELEASE);

    // Try to advance ring pointers
    if (task_id == sched->last_task_alive) {
        pto2_scheduler_advance_ring_pointers(sched); // RISK: Multiple entries
    }
}

int32_t pto2_scheduler_on_task_complete(PTO2SchedulerState* sched,
                                         int32_t task_id,
                                         uint64_t* ready_wait_cycles,
                                         uint64_t* ready_hold_cycles) {
    int32_t slot = sched->pto2_task_slot(task_id);
    PTO2TaskDescriptor* task = pto2_sm_get_task(sched->sm_handle, task_id);
    int32_t fanout_notified = 0;

    // === STEP 1: Mark COMPLETED and snapshot fanout_head under lock ===
    // Acquire fanout_lock to safely read fanout_head (orchestrator may be appending).
    // Release lock EARLY: once COMPLETED is visible, orchestrator's Step 5 will
    // skip this producer (prod_state >= COMPLETED), so no new entries can be
    // appended to the fanout list. Traversal outside the lock is safe.
    pto2_fanout_lock(task);
    __atomic_store_n(&sched->task_state[slot], PTO2_TASK_COMPLETED, __ATOMIC_RELEASE);
    __atomic_fetch_add(&sched->tasks_completed, 1, __ATOMIC_RELAXED);
    int32_t fanout_head = PTO2_LOAD_ACQUIRE(&task->fanout_head);
    pto2_fanout_unlock(task);

    // Traverse fanout chain OUTSIDE the lock to avoid blocking orchestrator
    int32_t current = fanout_head;
    while (current > 0) {
        PTO2DepListEntry* entry = pto2_dep_pool_get(sched->dep_pool, current);
        if (!entry) break;

        fanout_notified++;
        int32_t consumer_id = entry->task_id;
        PTO2TaskDescriptor* consumer = pto2_sm_get_task(sched->sm_handle, consumer_id);

        // Atomically increment consumer's fanin_refcount and check if consumer is now ready
        sched->release_fanin_and_check_ready(consumer_id, consumer, ready_wait_cycles, ready_hold_cycles);

        current = entry->next_offset;
    }

    // === STEP 2: Update fanout_refcount of all producers ===
    // This task is a consumer of its fanin producers - release references
    current = task->fanin_head;

    while (current > 0) {
        PTO2DepListEntry* entry = pto2_dep_pool_get(sched->dep_pool, current);
        if (!entry) break;

        int32_t producer_id = entry->task_id;
        pto2_scheduler_release_producer(sched, producer_id);

        current = entry->next_offset;
    }

    // === STEP 3: Check if this task can transition to CONSUMED ===
    check_and_handle_consumed(sched, task_id, task);
    return fanout_notified;
}

void pto2_scheduler_on_scope_end(PTO2SchedulerState* sched,
                                  const int32_t* task_ids, int32_t count) {
    for (int32_t i = 0; i < count; i++) {
        pto2_scheduler_release_producer(sched, task_ids[i]);
    }
}

void pto2_scheduler_release_producer(PTO2SchedulerState* sched, int32_t producer_id) {
    int32_t slot = sched->pto2_task_slot(producer_id);
    PTO2TaskDescriptor* producer = pto2_sm_get_task(sched->sm_handle, producer_id);

    // Increment fanout_refcount atomically (called from both orchestrator and scheduler threads)
    __atomic_fetch_add(&sched->fanout_refcount[slot], 1, __ATOMIC_ACQ_REL);

    // Check if producer can transition to CONSUMED
    check_and_handle_consumed(sched, producer_id, producer);
}

// =============================================================================
// Ring Pointer Management
// =============================================================================

void pto2_scheduler_advance_ring_pointers(PTO2SchedulerState* sched) {
    PTO2SharedMemoryHeader* header = sched->sm_handle->header;
    int32_t current_task_index = PTO2_LOAD_ACQUIRE(&header->current_task_index);

    // Advance last_task_alive while tasks at that position are CONSUMED
    while (sched->last_task_alive < current_task_index) {
        int32_t slot = sched->pto2_task_slot(sched->last_task_alive);

        if (sched->task_state[slot] != PTO2_TASK_CONSUMED) {
            break;  // Found non-consumed task, stop advancing
        }

        sched->last_task_alive++;
    }

    // Update heap_tail based on last consumed task's buffer
    if (sched->last_task_alive > 0) {
        int32_t last_consumed_id = sched->last_task_alive - 1;
        PTO2TaskDescriptor* last_consumed = pto2_sm_get_task(sched->sm_handle, last_consumed_id);

        if (last_consumed->packed_buffer_end != NULL) {
            sched->heap_tail = (uint64_t)((char*)last_consumed->packed_buffer_end - (char*)sched->heap_base);
        }
    }

    // Write to shared memory for orchestrator flow control
    pto2_scheduler_sync_to_sm(sched);
}

void pto2_scheduler_sync_to_sm(PTO2SchedulerState* sched) {
    PTO2SharedMemoryHeader* header = sched->sm_handle->header;

    PTO2_STORE_RELEASE(&header->last_task_alive, sched->last_task_alive);
    PTO2_STORE_RELEASE(&header->heap_tail, sched->heap_tail);
    // Keep generation in sync so AICPU mode sees a consistent starting state
    PTO2_STORE_RELEASE(&header->heap_tail_gen, sched->last_task_alive);
}

// =============================================================================
// Scheduler Main Loop Helpers
// =============================================================================

bool pto2_scheduler_is_done(PTO2SchedulerState* sched) {
    PTO2SharedMemoryHeader* header = sched->sm_handle->header;

    // Check if orchestrator has finished
    int32_t orch_done = PTO2_LOAD_ACQUIRE(&header->orchestrator_done);
    if (!orch_done) {
        return false;
    }

    // Check if all tasks have been consumed
    int32_t current_task_index = PTO2_LOAD_ACQUIRE(&header->current_task_index);
    return sched->last_task_alive >= current_task_index;
}

// =============================================================================
// Debug Utilities
// =============================================================================

void pto2_scheduler_print_stats(PTO2SchedulerState* sched) {
    LOG_INFO("=== Scheduler Statistics ===");
    LOG_INFO("last_task_alive:   %d", sched->last_task_alive);
    LOG_INFO("heap_tail:         %" PRIu64, sched->heap_tail);
    LOG_INFO("tasks_completed:   %lld", (long long)sched->tasks_completed);
    LOG_INFO("tasks_consumed:    %lld", (long long)sched->tasks_consumed);
    LOG_INFO("============================");
}

void pto2_scheduler_print_queues(PTO2SchedulerState* sched) {
    LOG_INFO("=== Ready Queues ===");

    const char* worker_names[] = {"CUBE", "VECTOR", "AI_CPU", "ACCELERATOR"};

    for (int i = 0; i < PTO2_NUM_WORKER_TYPES; i++) {
        LOG_INFO("  %s: count=%" PRIu64, worker_names[i],
                 pto2_ready_queue_count(&sched->ready_queues[i]));
    }

    LOG_INFO("====================");
}
